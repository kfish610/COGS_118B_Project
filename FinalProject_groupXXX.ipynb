{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "- Anna Sim\n",
    "- Jing Yin (Trevor) Yip\n",
    "- Kevin Fisher\n",
    "- Ashesh Kaji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In recent years, there has been a drastic increase in the implementation of large language models (LLM). LLMs accelerated the development of tools like recommendation systems, language sentiment analysis, and machine translation. The increased use of the language models requires further analysis and interpretation work to ensure responsible use of the architectures and models. High-risk environments require not only high accuracy of the predictions but also an understanding of why the model makes certain predictions<a name=\"molnar\"></a>[<sup>[4]</sup>](#molnarnote). As users rely more and more on machine learning systems to make decisions, there is an increasing risk of misuse and misinterpretation of the results and capabilities of the system. Therefore, there is a demand for interpretability research both in academia and industry.\n",
    "\n",
    "Interpretability of results of language models depends on understanding the relationships between semantic and syntactic content of words and their vector representations. Word2Vec is a popular algorithm developed by Tomas Mikolov et al. in 2013, which revolutionized the world of language models by proposing a new method of creating such continuous vector representations of words<a name=\"mikolov1\"></a>[<sup>[2]</sup>](#mikolov1note). Preliminary analysis of Word2Vec embeddings showed that the vector representation captures relationships between the words. In the original paper, the authors showed how similar words are not only represented with vectors that have high cosine similarity, but also exhibit complex relationships like vector(“smallest”) = vector(”biggest”) - vector(”big”) + vector(”small”). They also show intricate semantic relationships, like country/city relationships and syntactic relationships from adjectives to adverbs<a name=\"mikolov1\"></a>[<sup>[2]</sup>](#mikolov1note).\n",
    "\n",
    "In the follow-up paper, “Distributed Representations of Words and Phrases and their Compositionality,” Mikolov introduces an extension to the model, focusing on the word phrases and adding distributed representations of words and phrases, which allowed for even more robust vector representations and semantic relationships like idiomatic phrases<a name=\"mikolov2\"></a>[<sup>[3]</sup>](#mikolov2note). The paper showed connections from cities to their respective newspaper outlets and countries to their airlines<a name=\"mikolov2\"></a>[<sup>[3]</sup>](#mikolov2note).\n",
    "\n",
    "However, the research of Tal Linzen showed that there are potential limitations in relying on the cosine similarity for analogical reasoning to emerge, and the analogous relationships presented in the original papers break down when you reverse them<a name=\"linzen\"></a>[<sup>[1]</sup>](#linzennote). The analogical reasoning can be partially explained by the model picking up the closest neighbor word and not the relative difference in similarity in the analogous pair<a name=\"linzen\"></a>[<sup>[1]</sup>](#linzennote). Therefore, there is a need for further research looking into the relative neighborhoods and analogical reasoning capabilities of Word2Vec models. \n",
    "We will present an extensive analysis of the clustering of differences between pairs of Word2Vec embeddings, in order to examine the semantic and vector representation relationship of the words, gain insight into what drives the apparent vector-based analogical reasoning, and expand the potential areas of research on the interpretability of language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "It is well known that embeddings generated by Word2Vec models respect relationships between words. For example, the differences between the vectors for countries and their respective capitals are very similar between all such pairs. In this sense, we get “relationship vectors”, which capture an entire relationship with one vector, and allow for reasoning by analogy. However, most observations of this have been hand-picked, and have considered only a couple of possible relationships. As such, we would like to tackle the issue of investigating all possible “relationship vectors”, so that we are not leaving out potentially overlooked members of these relationships or overlooked relationships entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n",
    "\n",
    "# Data (answered)\n",
    "\n",
    "The dataset we used to obtain word pairs for analogies was the BATS 3.0 <a name=\"bats\"></a>[<sup>[5]</sup>](#batsnote). The BATS 3.0 dataset is a collection word pairs belonging to 4 major cateogries wherein each category as 10 sub-categories and 50 word pairs in each sub-category. These were accumulated using questionnaires and over 99,000 total questions. The dataset and its details, including the paper on it can be found at the following link:\n",
    "\n",
    "[https://aclweb.org/aclwiki/Bigger_analogy_test_set_(State_of_the_art)](https://aclweb.org/aclwiki/Bigger_analogy_test_set_(State_of_the_art))\n",
    "\n",
    "We will be using a set of [Word2Vec embeddings](https://code.google.com/archive/p/word2vec/), namely the [Google News negative sampling dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) that was trained in the original paper. This is the dataset in which analogical vector behavior was first identified, so it is the perfect candidate. Raw, each observation in this dataset is the 300-dimensional vector embedding of a word in the vocabulary, listed in frequency order. Each individual dimension doesn’t represent anything in particular, but together they can capture the relationships between large vocabularies of words. This particular dataset has a vocabulary of 3,000,000 words and phrases (observations), but since we are going to be looking at every pair of words, we will be restricting our data to the most frequent 10,000 points (roughly).\n",
    "\n",
    "Total observations: 2000 word pairs\n",
    "Format: Tab separated text file\n",
    "\n",
    "Data-cleaning: The data was in a clean format, but some pairs had multiple options for the latter word, so we chose the first option. Additionally, we removed any words that were not present in the google news word2vec model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file\n",
    "\n",
    "### Subsection 1 - Initial PCA with valid and invalid analogical relationship pairs\n",
    "### Subsection 2 - PCA on 3 valid analogical pair categories \n",
    "### Subsection 3 - PCA on 40 analogical categories from BATS dataset\n",
    "### Subsection 4 - Clustering of PCA results for 3 valid pair categories and 40 valid pair categories \n",
    "### Subsection 5 - Directionality of valid pair vectors under PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n",
    "<a name=\"batsnote\"></a>5.[^](#bats): Gladkova, A., Drozd, A., & Matsoukas, S. (2016). Analogy-based detection of morphological and semantic relations with word embeddings: What works and what doesn’t. In Proceedings of the NAACL-HLT SRW (pp. 47-54).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
